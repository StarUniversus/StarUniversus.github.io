@article{QUAN2024110745,
title = {Global contrast-masked autoencoders are powerful pathological representation learners},
journal = {Pattern Recognition},
volume = {156},
pages = {110745},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110745},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004965},
author = {Hao Quan and Xingyu Li and Weixing Chen and Qun Bai and Mingchen Zou and Ruijie Yang and Tingting Zheng and Ruiqun Qi and Xinghua Gao and Xiaoyu Cui},
keywords = {Self-supervised learning, Representation learning, Pathological image},
abstract = {Using digital pathology slide scanning technology, artificial intelligence algorithms, particularly deep learning, have achieved significant results in the field of computational pathology. Compared to other medical images, pathology images are more difficult to annotate, and thus, there is an extreme lack of available datasets for conducting supervised learning to train robust deep learning models. In this paper, we introduce a self-supervised learning (SSL) model, the Global Contrast-masked Autoencoder (GCMAE), designed to train encoders to capture both local and global features of pathological images and significantly enhance the performance of transfer learning across datasets. Our study demonstrates the capability of the GCMAE to learn transferable representations through extensive experiments on three distinct disease-specific hematoxylin and eosin (H&E)-stained pathology datasets: Camelyon16, NCT-CRC, and BreakHis. Moreover, we propose an effective automated pathology diagnosis process based on the GCMAE for clinical applications. The source code of this paper is publicly available at https://github.com/StarUniversus/gcmae.}
}